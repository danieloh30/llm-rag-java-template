
# RAG - Ingestion / Embedding configuration
quarkus.langchain4j.redis.dimension=1536
quarkus.redis.max-pool-waiting=128
quarkus.redis.max-pool-size=16
quarkus.redis.devservices.image-name=redis/redis-stack:latest

# LLM configuration
dev.quarkus.langchain4j.openai.log-requests=true
dev.quarkus.langchain4j.openai.log-responses=true

%dev.quarkus.langchain4j.openai.base-url=http://localhost:52707/v1
%dev.quarkus.langchain4j.openai.chat-model.model-name=instructlab/granite-7b-lab-GGUF
%dev.quarkus.langchain4j.openai.timeout=600s
%dev.quarkus.langchain4j.embedding-model.provider=dev.langchain4j.model.embedding.BgeSmallEnQuantizedEmbeddingModel

# Production configuration
%prod.quarkus.langchain4j.openai.base-url=https://openshift-ai-serving/llm/v1
%prod.quarkus.langchain4j.embedding-model.provider=dev.langchain4j.model.embedding.BgeSmallEnQuantizedEmbeddingModel

quarkus.smallrye-openapi.store-schema-directory=./