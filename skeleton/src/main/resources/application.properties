
# RAG - Ingestion / Embedding configuration
quarkus.langchain4j.redis.dimension=1536
quarkus.redis.max-pool-waiting=128
quarkus.redis.max-pool-size=16
quarkus.redis.devservices.image-name=redis/redis-stack:latest

# LLM - Chat model configuration

# To log request and responses.
# quarkus.langchain4j.openai.log-requests=true
# quarkus.langchain4j.openai.log-responses=true

# quarkus.langchain4j.openai.api-key=sk-dummy
%dev.quarkus.langchain4j.openai.base-url=http://localhost:61389/v1
# %dev.quarkus.langchain4j.openai.chat-model.model-name=TheBloke/Mistral-7B-Instruct-v0.1-GGUF
%dev.quarkus.langchain4j.openai.chat-model.model-name=instructlab/granite-7b-lab-GGUF
%dev.quarkus.langchain4j.openai.timeout=600s
quarkus.langchain4j.embedding-model.provider=dev.langchain4j.model.embedding.BgeSmallEnQuantizedEmbeddingModel

# Production configuration
# %prod.quarkus.langchain4j.openai.base-url=https://openshift-ai-serving/llm/v1

quarkus.smallrye-openapi.store-schema-directory=./